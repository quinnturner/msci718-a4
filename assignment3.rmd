---
title: "Assignment3"
output: html_document
---
```{r setup, include=FALSE}
#knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(pastecs)
library(car)
```

### Team Members
* Quinn Turner // sqturner // 20576575 - Exploratory Data & ANOVA
* Rebecca Rayner // rsrayner // 20562488 - Linear Regression & Two Means
* Yusuf Khaled // yhbkhale // 20508841 - Logistic Regression
* Samantha Villaluz // skvillal // 20582656 - Linear Regression & Two Means

### Assignment Breakdown
1. Exploratory Data Analysis
    + Normally Distributed Variables
    + Correlation using Levene's Test
2. Hypothesis
    + Hypothesis #1: Low acidity and sugar content are major indicators of wine quality (Linear regression and ANOVA)
    + Hypothesis #2: Acidity is a strong predictor of wine colour (Logistic & linear regression)
    + Hypothesis #3: Red wine is of higher quality (Comparison of two means)

## Set up datasets
```{r}
redWineData <- read.csv("winequality-red.csv", header=T, sep = ";")
# Reduces 1599 observations to 1359 observations
redWineData <- redWineData[!duplicated(redWineData), ]
redWineData$color <- 1
redWineData$pHTransformed <- 10^redWineData$pH
redWineData$pH <- NULL
redWineData$quality <- factor(redWineData$quality, ordered = T)

head(whiteWineData)

whiteWineData <- read.csv("winequality-white.csv", header=T, sep = ";")
# Reduces 4898 observations to 3961 observations
whiteWineData <- whiteWineData[!duplicated(whiteWineData), ]
whiteWineData$color <- 0
whiteWineData$pHTransformed <- 10^whiteWineData$pH
whiteWineData$pH <- NULL
whiteWineData$quality <- factor(whiteWineData$quality, ordered = T)

head(redWineData)

wineData <- rbind(whiteWineData, redWineData)

str(wineData)
```


## Exploratory Data Analysis

We begin by determining the underlying distribution for each variable within the wine dataset. This will allow us to determine which tests and analyses we can perform on what variables, as well as getting a feel for the data.

```{r}
# Per-variable histogram
ggplot(wineData, aes(fixed.acidity)) + geom_histogram(aes(y=..density..), binwidth = .1, color="darkblue", fill="lightblue") + labs(title = 'Histogram of fixed acidity', x = 'Fixed acidity') + geom_vline(aes(xintercept=mean(fixed.acidity)), color="red", linetype="dashed", size=1) + geom_density(alpha=.2, fill="#FF6666")

ggplot(wineData, aes(volatile.acidity)) + geom_histogram(aes(y=..density..), binwidth = .1, color="darkblue", fill="lightblue") + labs(title = 'Histogram of volatile acidity', x = 'Volatile acidity') + geom_vline(aes(xintercept=mean(volatile.acidity)), color="red", linetype="dashed", size=1) + geom_density(alpha=.2, fill="#FF6666")

ggplot(wineData, aes(citric.acid)) + geom_histogram(aes(y=..density..), binwidth = .1, color="darkblue", fill="lightblue") + labs(title = 'Histogram of citric acid', x = 'Citric acid') + geom_vline(aes(xintercept=mean(citric.acid)), color="red", linetype="dashed", size=1) + geom_density(alpha=.2, fill="#FF6666")

ggplot(wineData, aes(residual.sugar)) + geom_histogram(aes(y=..density..), binwidth = 1, color="darkblue", fill="lightblue") + labs(title = 'Histogram of residual sugar', x = 'Residual sugar') + geom_vline(aes(xintercept=mean(residual.sugar)), color="red", linetype="dashed", size=1) + geom_density(alpha=.2, fill="#FF6666")

ggplot(wineData, aes(chlorides)) + geom_histogram(aes(y=..density..), binwidth = .01, color="darkblue", fill="lightblue") + labs(title = 'Histogram of chlorides', x = 'Chlorides') + geom_vline(aes(xintercept=mean(chlorides)), color="red", linetype="dashed", size=1) + geom_density(alpha=.2, fill="#FF6666")

ggplot(wineData, aes(free.sulfur.dioxide)) + geom_histogram(aes(y=..density..), binwidth = 5, color="darkblue", fill="lightblue") + labs(title = 'Histogram of free sulfur dioxide', x = 'Free sulfur dioxide') + geom_vline(aes(xintercept=mean(free.sulfur.dioxide)), color="red", linetype="dashed", size=1) + geom_density(alpha=.2, fill="#FF6666")

ggplot(wineData, aes(total.sulfur.dioxide)) + geom_histogram(aes(y=..density..), binwidth = 5, color="darkblue", fill="lightblue") + labs(title = 'Histogram of total sulfur dioxide', x = 'Total sulfur dioxide') + geom_vline(aes(xintercept=mean(total.sulfur.dioxide)), color="red", linetype="dashed", size=1) + geom_density(alpha=.2, fill="#FF6666")

ggplot(wineData, aes(density)) + geom_histogram(aes(y=..density..), binwidth = .001, color="darkblue", fill="lightblue") + labs(title = 'Histogram of density', x = 'Density') + geom_vline(aes(xintercept=mean(density)), color="red", linetype="dashed", size=1) + geom_density(alpha=.2, fill="#FF6666")

ggplot(wineData, aes(sulphates)) + geom_histogram(aes(y=..density..), binwidth = .1, color="darkblue", fill="lightblue") + labs(title = 'Histogram of sulphates', x = 'Sulphates') + geom_vline(aes(xintercept=mean(sulphates)), color="red", linetype="dashed", size=1) + geom_density(alpha=.2, fill="#FF6666")

ggplot(wineData, aes(alcohol)) + geom_histogram(aes(y=..density..), binwidth = .5, color="darkblue", fill="lightblue") + labs(title = 'Histogram of alcohol', x = 'Alcohol') + geom_vline(aes(xintercept=mean(alcohol)), color="red", linetype="dashed", size=1) + geom_density(alpha=.2, fill="#FF6666")

ggplot(wineData, aes(quality)) + geom_histogram(aes(y=..density..), binwidth = 1, color="darkblue", fill="lightblue") + labs(title = 'Histogram of quality', x = 'Quality') + geom_vline(aes(xintercept=mean(quality)), color="red", linetype="dashed", size=1)

ggplot(wineData, aes(color)) + geom_histogram(binwidth = 1, color="darkblue", fill="lightblue") + labs(title = 'Histogram of color', x = 'Color')

ggplot(wineData, aes(pHTransformed)) + geom_histogram(aes(y=..density..), binwidth = 100, color="darkblue", fill="lightblue") + labs(title = 'Histogram of pH transformed', x = 'pH transformed') + geom_vline(aes(xintercept=mean(pHTransformed)), color="red", linetype="dashed", size=1) + geom_density(alpha=.2, fill="#FF6666")
```

We now have have an glimpse of understanding for the distribution of each variable for wine. These histograms include both red and white wine in one plot. Most variables are positively skewed.

For a closer look at normality, we can show QQ plots. To reduce variance, we can view just white wine alone.

```{r}
qqPlot(whiteWineData$fixed.acidity)
qqPlot(whiteWineData$volatile.acidity)
qqPlot(whiteWineData$citric.acid)
qqPlot(whiteWineData$residual.sugar)
qqPlot(whiteWineData$chlorides)
qqPlot(whiteWineData$free.sulfur.dioxide)
qqPlot(whiteWineData$total.sulfur.dioxide)
qqPlot(whiteWineData$total.sulfur.dioxide)
qqPlot(whiteWineData$density)
qqPlot(whiteWineData$sulphates)
qqPlot(whiteWineData$alcohol)
qqPlot(whiteWineData$quality)
qqPlot(whiteWineData$pHTransformed)
```

From these graphs, the following variables could be argued to have an underlying normally distribution:
* fixed.acidity
* citric.acid
* chlorides (semi bimodal)
* sulphates
* quality
* pH (transformed)
* density

The following are not normally distributed
* residual sugar (positive skew)
* total sulphur dioxide (bimodal)
* alcohol

From the QQ plots, most variables are right-skewed and none of them are perfectly normal

## Testing for Correlation
```{r}

#broken when run pls fix
#leveneTest(wineData$fixed.acidity, wineData$quality)
#leveneTest(wineData$citric.acid, wineData$quality)
#leveneTest(wineData$chlorides, wineData$quality)
#leveneTest(wineData$sulphates, wineData$quality)
#leveneTest(wineData$pH, wineData$quality)

cor(wineData, method="kendall")
```


From Levene's test (of the variables found to be normal above), we find that citric.acid, chlorides, sulphates and pH are homoscedastic amongst varying levels of quality, whereas we cannot conclude the same for fixed.acidity.

## Hypotheses

### 1. What factors impact wine quality? Hypothesis: Low acidity and sugar content are major indicators of wine quality.

#### 1.1 Linear Regression
#####Assumptions for Linear Regression
1. All predictor variables must be quantitative or categorical, and outcome must be quantitative, continuous, and unbounded (Y)
2. Non-zero variance (Y)
3. No perfect multicollinearity (predictor variables should not correlate highly)
4. Predictors are uncorrelated with external variables (Y - we have to assume)
5. Residuals are homoscedastic (constant variance), independent (test with Durbin-Watson), Normal
6. Linearity (outcome variable means lie on straight line)

First, we perform linear regression off twelve variables to predict the outcome variable: quality. By running the lm command, we receive co-efficient estimates, standard error, t-value and p-values.

```{r}
wineQuality.model <- lm(quality ~ fixed.acidity + volatile.acidity + citric.acid + residual.sugar + chlorides + free.sulfur.dioxide + total.sulfur.dioxide + density + pHTransformed + sulphates + alcohol + color, data=wineData)
summary(wineQuality.model)
```

```{r}
ggplot(data=wineData, aes(x=quality, y=volatile.acidity, group=quality)) +
  geom_boxplot() +
  geom_jitter(width=0.1, height=0.5, aes(color="red"))
```

It seems that highly acidic wines tend to be of lower quality, though there is significant variance.

```{r}
ggplot(data=wineData, aes(x=quality, y=residual.sugar, group=quality)) +
  geom_boxplot() +
  geom_jitter(width=0.1, height=0.5, aes(color="red"))
```

By itself, residual sugar level does not seem to be correlated with the quality of the wine.

```{r}
ggplot(wineData, aes(y=quality, x=density)) + geom_jitter(width=0.1) + facet_grid(~color)
```

#### Interpretation of Results:
Glancing at the values received from the estimate, density has the largest negative correlation with quality (-83), with acidity following after (-1.5). Each have a p-value of less than 0.05. One can gather, when separating by colour especially, that density is a weak indicator of quality of wine especially since the R squared value obtained is insufficient.


### 2. What factors predict wine colour? Hypothesis: Acidity is a strong predictor of wine colour (i.e. red vs. white)

#### 2.1 Linear Regression

```{r}
# Note, the parameters provided do not include quality since this is a subjective measure.
wineColor.modelFull <- lm(color ~ fixed.acidity + volatile.acidity + citric.acid + residual.sugar + chlorides + free.sulfur.dioxide + total.sulfur.dioxide + density + pHTransformed + sulphates + alcohol, data=wineData)
summary(wineColor.modelFull)

# Removed variables with low t-values
wineColor.modelReduced <- lm(color ~ fixed.acidity + volatile.acidity + residual.sugar + chlorides + total.sulfur.dioxide + density + alcohol, data=wineData)
summary(wineColor.modelReduced)

Anova(wineColor.modelFull, type = 3)

Anova(wineColor.modelReduced, type = 3)
```

#### Interpretation of Results:
Density has the largest positive correlation with wine colour (co-efficient = 13.47), with chlorides following after (co-efficient = 0.76). Each have a p-value of less than 0.05. Contradictory to our hypothesis, fixed and volatile acidity were not strong predictors of wine colour.

#### 2.2 Logistic Regression
```{r}
#Lets use Logistic Regression to try and predict wine colour!

# We'll want to use some train-test split on the data to validate our logistic regression model.
wineDataShuffled <- wineData[sample(nrow(wineData)),]
wineLength <- nrow(wineDataShuffled)  # 6497
wineLengthEightyP <- as.integer(nrow(wineDataShuffled)*0.8)  # 5197
#create train and test set; 60%-40% split
train = wineDataShuffled[1:5197,]
test = wineDataShuffled[5198:6497,]

# Lets check out a bunch of feature variables
glm.fit1 = glm(color ~ pHTransformed + volatile.acidity + chlorides + total.sulfur.dioxide, data = train, family = binomial)
summary(glm.fit1)

# Interestingly, all of these variables are shown to be significant, as their p-values are close to 0
# This is a good sign that our model could be a good predictor.
glmProbTrain = predict(glm.fit1, newdata = train, type = "response")
# Use these probabilities to predict red vs wine
glmPredictTrain = ifelse(glmProbTrain > 0.5, "red", "white")
# Lets use a confusion matrix to see how the logistic regression model performed
trainOutcomeValues = cbind(train$color)
trainOutcomeValues = ifelse(trainOutcomeValues == 1, "red", "white")
# Lets see our confusion matrix and accuracy values
confMatTrain <- table(glmPredictTrain, trainOutcomeValues)
# Accuracy:
mean(glmPredictTrain == trainOutcomeValues)

# Now let's compare this to our test data
glmProbTest = predict(glm.fit1, newdata = test, type = "response")
# Use these probabilities to predict red vs wine
glmPredictTest = ifelse(glmProbTest > 0.5, "red", "white")
# Lets use a confusion matrix to see how the logistic regression model performed
testOutcomeValues = cbind(test$color)
testOutcomeValues = ifelse(testOutcomeValues == 1, "red", "white")
# Lets see our confusion matrix and accuracy values
confMat <- table(glmPredictTest, testOutcomeValues)
# Accuracy:
mean(glmPredictTest == testOutcomeValues)

```


#### Interpretation of Results:
As we can see, the logistic regression model produces quite good results: 97.3% accuracy over training data and 97.0% accuracy over the testing data. It is a good sign that the accuracy barely drops between training and testing data, as this ensures the model hasn't overfit to the learned data. Based on the fact that all of the feature variables used in the model (pH, volatile acidity, chlorides, sulfur dioxide) were shown to be significantly related to our predictor variable in the summary of the model's fit, it makes sense that these variables would form a well-performing logistic regression model.

#####Assumptions for Logistic Regression
1. Linearity: linear relationship between continuous predictors and the logit of the outcome variable (see section 8.8.1)
2. Independence of errors: same as linear regression
3. Multicollinearity: same as linear regression
plus, there are some unique problems of its own:
1. Incomplete information: need full combinations of variables
2. Complete separation: logistic regression fails if your data does not overlap


### 3. Which wine is better quality: red or white? Hypothesis: Red wine is of higher quality.

##### t-test Assumptions
1. Sampling distribution is normally distributed
--> based on visual inspection, quality appears to be relatively normally distributed
Can't use stat_desc because there are too many datapoints
2. Data are at least interval (Yes, when not-factored)

```{r}
t.test(as.numeric(redWineData$quality), as.numeric(whiteWineData$quality))

line <- ggplot(wineData, aes(color, as.numeric(quality)))
line + stat_summary(fun.y = mean, geom = "line", size = 1, aes(group=1), colour = "#FF6633") + stat_summary(fun.data = mean_cl_boot, geom = "errorbar", width = 0.2, size = 0.75, colour = "#990000") + stat_summary(fun.y = mean, geom = "point", size = 4, colour = "#990000") + stat_summary(fun.y = mean, geom = "point", size = 3, colour = "#FF6633") + labs(x = "color", y = "Quality")
```

#### Interpretation of Results:

### Assumptions for Logistic Regression
1.linearity: linear relationship between continuous predictors and the logit of the outcome variable (see section 8.8.1)
2.independence of errors: same as linear regression
3.multicollinearity: same as linear regression
plus, there are some unique problems of its own:
1.incomplete information: need full combinations of variables
2.complete separation: logistic regression fails if your data does not overlap


### 4.ANOVA


We would like to look if alcohol differs among quality types. We use one-way ANOVA this time.

```{r}
wineAovModel<-aov(alcohol~quality, data = wineData)
summary(wineAovModel)
```

#### Interpretation of Results:

The result of the one-way ANOVA show significant difference in alcohol content among quality types. This is clear by the high F-statistic and small p-value.
